{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Text Similarity\n",
    "So... I came across a job posting on Handshake by Berkeley Haas that required the use of Google's Bidirectional Encoder Representations (BERT) from Transformers. BERT is designed to understand the context and meaning of words in a sentence by considering the words that come before and after them. We'll dive deeper into what transformers are and how exactly do you train the model for text classification. \n",
    "\n",
    "In this case, we're looking at how we can use BERT to detect if two texts are similar to one another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\qmirz\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "train_file = bz2.BZ2File(\"./data/train.ft.txt.bz2\")\n",
    "test_file = bz2.BZ2File(\"./data/test.ft.txt.bz2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store sentences and labels as lists\n",
    "train_lines = train_file.readlines()\n",
    "test_lines = test_file.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use the bz2 module, the readlines() method returns the content of the file as a list of bytes objects (raw binary strings). For processing the data more easily, we convert the byte objects into strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lines = [x.decode('utf-8') for x in train_lines]\n",
    "test_lines = [x.decode('utf-8') for x in test_lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've loaded our data in string format, we can start preprocessing the data. Here's what we will do:\n",
    "\n",
    "1. Split label from sentence\n",
    "2. tokenization\n",
    "3. Punctuation removal\n",
    "4. Removing stop words\n",
    "5. lowercasing\n",
    "6. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get labels: 0 if __label__1, 1 if __label__2\n",
    "train_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_lines]\n",
    "train_sentences = [x.split(' ', 1)[1][:-1].lower() for x in train_lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To break the code down:\n",
    "- the first line checks each train line for '__label__1', and assigns a target value of 0. If we find '__label__2', assign target value to 1. \n",
    "\n",
    "- In the 2nd line, we remove the label using x.split('', 1)[1] which creates a list with 2 elements (looks something like ['__label__1', 'sentence']) and selects the element indexed at 1 which effectively removes the label at the beginning of the sentence.\n",
    "\n",
    "- The [:-1] serves to remove \"\\n\" which is the newline character at the end of each line\n",
    "\n",
    "Finally we call .lower() to those sentences to standardize the data and avoid duplication of words due to case differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import regular expressions module for pattern matching and string manipulation\n",
    "import re\n",
    "for i in range(len(train_sentences)):\n",
    "    # remove non-alphabetic characters like numbers\n",
    "    train_sentences[i] = re.sub('\\d','0',train_sentences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we're doing here is iterating through the indices of train_sentences. For each train_sentence, we substitute any num-alphabetic character with the digit 0. To break down the re.sub() function:\n",
    "\n",
    "- Arg 1 ('\\d'): regular expression that matches any digit\n",
    "- Arg 2 (0): Replace matched digits with 0\n",
    "- Arg 3 (train_sentences[i]): Input train_sentences[i]\n",
    "\n",
    "The main reason for this process is to:\n",
    "1. Improve Model Generalization\n",
    "2. Noise Reduction\n",
    "3. Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we do the same thing for the test data\n",
    "test_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test_lines]\n",
    "test_sentences = [x.split(' ', 1)[1][:-1].lower() for x in test_lines]\n",
    "\n",
    "for i in range(len(test_sentences)):\n",
    "    test_sentences[i] = re.sub('\\d','0',test_sentences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next type of data cleaning we should do is to remove URLs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_sentences)):\n",
    "    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n",
    "        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above loops through the train sentences, checks for ['www.', 'http:', 'https:', '.com'] in the train sentences. If any of these exists, it will look for any sequence of characters that:\n",
    "\n",
    "1. Does not contain a space\n",
    "2. ends with a period followed by 3 characters\n",
    "\n",
    "Which is essentially a url! we then replace that entire url with < url >. Again we do the same thing for the test data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_sentences)):\n",
    "    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n",
    "        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the best soundtrack ever to anything.: i'm reading a lot of reviews saying that this is the best 'game soundtrack' and i figured that i'd write a review to disagree a bit. this in my opinino is yasunori mitsuda's ultimate masterpiece. the music is timeless and i'm been listening to it for years now and its beauty simply refuses to fade.the price tag on this is pretty staggering i must say, but if you are going to buy any cd for this much money, this is the only one that i feel would be worth every penny.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright now that we've cleaned the data, we can proceed with Tokenization.\n",
    "\n",
    "Tokenization is the process of breaking down text into smaller pieces called tokens. For example, if you want to tokenize a sentence, the individual tokens could be the words that make up the sentence. This helps convert the sentence into a format that is easier for a machine to understand and process the text. \n",
    "\n",
    "In fact this is a necessary step before we can apply any other preprocessing algorithm like lemmatization or removing stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the tokenizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# set the maximum number of words to 20000\n",
    "max_words = 20000\n",
    "\n",
    "# create a tokenizer object\n",
    "tokenizer = Tokenizer(num_words=max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the tokenizer to the training data\n",
    "tokenizer.fit_on_texts(train_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our tokenized text, the next step if convert the tokenized text into sequences. Now the main reason for this is as follows:\n",
    "\n",
    "1. Input Formatting: Models don't understand text but they understand numbers. When we do this conversion, we convert thet text into sequence of numbers where each number represents a specific word. We can then feed our converted data into the model\n",
    "\n",
    "2. Maintaining Context: This conversion allows us to maintain the order of words which will be important for the model to understand context and the meaning of sentences. This is also important for models like Recurrent Neural Networks and Transformers (like BERT) which consider the order of words.\n",
    "\n",
    "3. Embedding: Once we have text data in sequence format, we can use word embedding to convert the sequences into dense vectors of fixed size which capture the semantic properties of the words. I'll explain this in further detail later! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the training data to sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
